{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import traceback\n",
    "import functools\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pytz\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from astropy import stats\n",
    "from astropy.io import fits, ascii\n",
    "from astropy.table import Column\n",
    "\n",
    "import astropy.units as u\n",
    "from astropy.io import fits\n",
    "from mmtwfs.wfs import WFSFactory\n",
    "\n",
    "tz = pytz.timezone(\"America/Phoenix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traceback(f):\n",
    "    @functools.wraps(f)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except Exception as ex:\n",
    "            ret = '#' * 60\n",
    "            ret += \"\\nException caught:\"\n",
    "            ret += \"\\n\"+'-'*60\n",
    "            ret += \"\\n\" + traceback.format_exc()\n",
    "            ret += \"\\n\" + '-' * 60\n",
    "            ret += \"\\n\"+ \"#\" * 60\n",
    "            print(sys.stderr, ret)\n",
    "            sys.stderr.flush()\n",
    "            raise ex\n",
    " \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate all of the WFS systems...\n",
    "wfs_keys = ['f9', 'newf9', 'f5', 'mmirs', 'binospec']\n",
    "wfs_systems = {}\n",
    "wfs_names = {}\n",
    "for w in wfs_keys:\n",
    "    wfs_systems[w] = WFSFactory(wfs=w)\n",
    "    wfs_names[w] = wfs_systems[w].name\n",
    "plt.close('all')\n",
    "\n",
    "# give mmirs a default\n",
    "wfs_systems['mmirs'].default_mode = 'mmirs1'\n",
    "\n",
    "# map f9 to oldf9\n",
    "wfs_systems['oldf9'] = wfs_systems['f9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_image(f, wfskey=None):\n",
    "    hdr = {}\n",
    "    with fits.open(f) as hdulist:\n",
    "        for h in hdulist:\n",
    "            hdr.update(h.header)\n",
    "        data = hdulist[-1].data\n",
    "    \n",
    "    # if wfskey is None, figure out which WFS from the header info...\n",
    "    if wfskey is None:\n",
    "        # check for MMIRS\n",
    "        if 'WFSNAME' in hdr:\n",
    "            if 'mmirs' in hdr['WFSNAME']:\n",
    "                wfskey = 'mmirs'\n",
    "        if 'mmirs' in f.name:\n",
    "            wfskey = 'mmirs'\n",
    "\n",
    "        # check for binospec\n",
    "        if 'bino' in f.name:\n",
    "            wfskey = 'binospec'\n",
    "        if 'ORIGIN' in hdr:\n",
    "            if 'Binospec' in hdr['ORIGIN']:\n",
    "                wfskey = 'binospec'\n",
    "\n",
    "        # check for new F/9\n",
    "        if 'f9wfs' in f.name:\n",
    "            wfskey = 'newf9'\n",
    "        if 'OBSERVER' in hdr:\n",
    "            if 'F/9 WFS' in hdr['OBSERVER']:\n",
    "                wfskey = 'newf9'\n",
    "        if wfskey is None and 'CAMERA' in hdr:\n",
    "            if 'F/9 WFS' in hdr['CAMERA']:\n",
    "                wfskey = 'newf9'\n",
    "\n",
    "        # check for old F/9\n",
    "        if 'INSTRUME' in hdr:\n",
    "            if 'Apogee' in hdr['INSTRUME']:\n",
    "                wfskey = 'oldf9'\n",
    "        if 'DETECTOR' in hdr:\n",
    "            if 'Apogee' in hdr['DETECTOR']:\n",
    "                wfskey = 'oldf9'\n",
    "\n",
    "        # check for F/5 (hecto)\n",
    "        if wfskey is None and 'SEC' in hdr:  # mmirs has SEC in header as well and is caught above\n",
    "            if 'F5' in hdr['SEC']:\n",
    "                wfskey = 'f5'\n",
    "        if Path(f.parent / \"F5\").exists():\n",
    "            wfskey = 'f5'\n",
    "            \n",
    "    if wfskey is None:\n",
    "        # if wfskey is still None at this point, whinge.\n",
    "        print(f\"Can't determine WFS for {f.name}...\")\n",
    "\n",
    "    if 'AIRMASS' not in hdr:\n",
    "        if 'SECZ' in hdr:\n",
    "            hdr['AIRMASS'] = hdr['SECZ']\n",
    "        else:\n",
    "            hdr['AIRMASS'] = np.nan\n",
    "\n",
    "    if 'EXPTIME' not in hdr:\n",
    "        hdr['EXPTIME'] = np.nan\n",
    "            \n",
    "    # we need to fix the headers in all cases to have a proper DATE-OBS entry with\n",
    "    # properly formatted FITS timestamp.  in the meantime, this hack gets us what we need \n",
    "    # for analysis in pandas.\n",
    "    dtime = None\n",
    "    if 'DATEOBS' in hdr:\n",
    "        dateobs = hdr['DATEOBS']\n",
    "        if 'UT' in hdr:\n",
    "            ut = hdr['UT'].strip()\n",
    "        elif 'TIME-OBS' in hdr:\n",
    "            ut = hdr['TIME-OBS']\n",
    "        else:\n",
    "            ut = \"07:00:00\"  # midnight\n",
    "        timestring = dateobs + \" \" + ut + \" UTC\"\n",
    "        if '-' in timestring:\n",
    "            dtime = datetime.strptime(timestring, \"%Y-%m-%d %H:%M:%S %Z\")\n",
    "        else:\n",
    "            dtime = datetime.strptime(timestring, \"%a %b %d %Y %H:%M:%S %Z\")\n",
    "\n",
    "    else:\n",
    "        if wfskey == \"oldf9\":\n",
    "            d = hdr['DATE-OBS']\n",
    "            if '/' in d:\n",
    "                day, month, year = d.split('/')\n",
    "                year = str(int(year) + 1900)\n",
    "                timestring = year + \"-\" + month + \"-\" + day + \" \" + hdr['TIME-OBS'] + \" UTC\"\n",
    "            else:\n",
    "                timestring = d + \" \" + hdr['TIME-OBS'] + \" UTC\"\n",
    "            dtime = datetime.strptime(timestring, \"%Y-%m-%d %H:%M:%S %Z\")\n",
    "        else:\n",
    "            if 'DATE-OBS' in hdr:\n",
    "                timestring = hdr['DATE-OBS'] + \" UTC\"\n",
    "                try:\n",
    "                    dtime = datetime.strptime(timestring, \"%Y-%m-%dT%H:%M:%S.%f %Z\")\n",
    "                except:\n",
    "                    dtime = datetime.strptime(timestring, \"%Y-%m-%dT%H:%M:%S %Z\")\n",
    "                # mmirs uses local time in this header pre-2019\n",
    "                if wfskey == 'mmirs' and dtime < datetime.fromisoformat(\"2019-01-01T12:00:00\"):\n",
    "                    local_dt = tz.localize(dtime)\n",
    "                    dtime = local_dt.astimezone(pytz.utc)\n",
    "            else:\n",
    "                dt = datetime.fromtimestamp(f.stat().st_ctime)\n",
    "                local_dt = tz.localize(dt)\n",
    "                dtime = local_dt.astimezone(pytz.utc)\n",
    "\n",
    "    if dtime is None:\n",
    "        print(f\"No valid timestamp in header for {f.name}...\")\n",
    "        obstime = None\n",
    "    else:\n",
    "        obstime = dtime.isoformat().replace('+00:00', '')\n",
    "        \n",
    "    hdr['WFSKEY'] = wfskey\n",
    "    hdr['OBS-TIME'] = obstime\n",
    "    return data, hdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "@get_traceback\n",
    "def process_image(f):\n",
    "    \"\"\"\n",
    "    Process FITS file, f, to get info we want from the header and then analyse it with the \n",
    "    appropriate WFS instance. Return results in a comma-separated line that will be collected \n",
    "    and saved in a CSV file.\n",
    "    \"\"\"            \n",
    "    data, hdr = check_image(f)\n",
    "    \n",
    "    wfskey = hdr['WFSKEY']\n",
    "    obstime = hdr['OBS-TIME']\n",
    "    airmass = hdr['AIRMASS']\n",
    "    exptime = hdr['EXPTIME']\n",
    "    az = hdr.get('AZ', np.nan)\n",
    "    el = hdr.get('EL', np.nan)\n",
    "    tiltx = hdr.get('TILTX', np.nan)\n",
    "    tilty = hdr.get('TILTY', np.nan)\n",
    "    transx = hdr.get('TRANSX', np.nan)\n",
    "    transy = hdr.get('TRANSY', np.nan)\n",
    "    focus = hdr.get('FOCUS', np.nan)\n",
    "    if np.isnan(focus) and 'TRANSZ' in hdr:\n",
    "        focus = hdr.get('TRANSZ', np.nan)\n",
    "    osst = hdr.get('OSSTEMP', np.nan)\n",
    "    if 'OUT_T' in hdr:\n",
    "        outt = hdr.get('OUT_T', np.nan)\n",
    "    else:\n",
    "        outt = hdr.get('T_OUT', np.nan)\n",
    "    if 'CHAM_T' in hdr:\n",
    "        chamt = hdr.get('CHAM_T', np.nan)\n",
    "    else:\n",
    "        chamt = hdr.get('T_CHAM', np.nan)\n",
    "    \n",
    "    # being conservative here and only using data that has proper slope determination\n",
    "    # and wavefront solution. also want to get statistics on the quality of the wavefront fits.\n",
    "    try:\n",
    "        results = wfs_systems[wfskey].measure_slopes(str(f), plot=False)\n",
    "    except:\n",
    "        print(f\"Problem analyzing {f.name}...\")\n",
    "        results = {}\n",
    "        results['slopes'] = None\n",
    "\n",
    "    if results['slopes'] is not None:\n",
    "        zresults = wfs_systems[wfskey].fit_wavefront(results, plot=False)\n",
    "        zv = zresults['zernike']\n",
    "        focerr = wfs_systems[wfskey].calculate_focus(zv)\n",
    "        cc_x_err, cc_y_err = wfs_systems[wfskey].calculate_cc(zv)\n",
    "        line = f\"{obstime},{wfskey},{f.name},{exptime},{airmass},{az},{el},{osst},{outt},{chamt},{tiltx},{tilty},{transx},{transy},{focus},{focerr.value},{cc_x_err.value},{cc_y_err.value},{results['xcen']},{results['ycen']},{results['seeing'].value},{results['raw_seeing'].value},{results['fwhm']},{zresults['zernike_rms'].value},{zresults['residual_rms'].value}\\n\"\n",
    "        zfile = f.parent / (f.stem + \".reanalyze.zernike\")\n",
    "        zresults['zernike'].save(filename=zfile)\n",
    "        spotfile = f.parent / (f.stem + \".spots.csv\")\n",
    "        results['spots'].write(spotfile, overwrite=True)\n",
    "        return line\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#rootdir = Path(\"/Users/tim/MMT/wfsdat\")\n",
    "rootdir = Path(\"/Volumes/LaCie 8TB/wfsdat\")\n",
    "#rootdir = Path(\"/Volumes/Seagate2TB/wfsdat\")\n",
    "#rootdir = Path(\"/mnt/d/wfsdat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already processed 20190101...\n",
      "Already processed 20190102...\n",
      "Already processed 20190103...\n",
      "Already processed 20190104...\n",
      "Already processed 20190105...\n",
      "Already processed 20190106...\n",
      "Already processed 20190107...\n",
      "Already processed 20190108...\n",
      "Already processed 20190109...\n",
      "Already processed 20190110...\n",
      "Already processed 20190111...\n",
      "Already processed 20190112...\n",
      "Already processed 20190113...\n",
      "Already processed 20190114...\n",
      "Already processed 20190115...\n",
      "Already processed 20190116...\n",
      "Already processed 20190117...\n",
      "Already processed 20190118...\n",
      "Already processed 20190119...\n",
      "Already processed 20190120...\n",
      "Already processed 20190121...\n",
      "Already processed 20190122...\n",
      "Already processed 20190123...\n",
      "Already processed 20190124...\n",
      "Already processed 20190125...\n",
      "Already processed 20190126...\n",
      "Already processed 20190127...\n",
      "Already processed 20190128...\n",
      "Already processed 20190129...\n",
      "Already processed 20190130...\n",
      "Already processed 20190131...\n",
      "Already processed 20190201...\n",
      "Already processed 20190202...\n",
      "Already processed 20190203...\n",
      "Already processed 20190204...\n",
      "Already processed 20190205...\n",
      "Already processed 20190206...\n",
      "Already processed 20190207...\n",
      "Already processed 20190208...\n",
      "Already processed 20190209...\n",
      "Already processed 20190210...\n",
      "Already processed 20190211...\n",
      "Already processed 20190212...\n",
      "Already processed 20190213...\n",
      "Already processed 20190214...\n",
      "Already processed 20190215...\n",
      "Already processed 20190216...\n",
      "Already processed 20190217...\n",
      "Already processed 20190218...\n",
      "Already processed 20190219...\n",
      "Already processed 20190220...\n",
      "Already processed 20190221...\n",
      "Already processed 20190222...\n",
      "Already processed 20190223...\n",
      "Already processed 20190224...\n",
      "Already processed 20190225...\n",
      "Already processed 20190226...\n",
      "Already processed 20190227...\n",
      "Already processed 20190228...\n",
      "Already processed 20190301...\n",
      "Already processed 20190302...\n",
      "Already processed 20190303...\n",
      "Already processed 20190304...\n",
      "Already processed 20190305...\n",
      "Already processed 20190306...\n",
      "Already processed 20190307...\n",
      "Already processed 20190308...\n",
      "Already processed 20190309...\n",
      "Already processed 20190310...\n",
      "Already processed 20190311...\n",
      "Already processed 20190312...\n",
      "Already processed 20190313...\n",
      "Already processed 20190314...\n",
      "Already processed 20190315...\n",
      "Already processed 20190316...\n",
      "Already processed 20190317...\n",
      "Already processed 20190318...\n",
      "Already processed 20190319...\n",
      "Already processed 20190320...\n",
      "Already processed 20190321...\n",
      "Already processed 20190322...\n",
      "Already processed 20190323...\n",
      "Already processed 20190324...\n",
      "Already processed 20190325...\n",
      "Already processed 20190326...\n",
      "Already processed 20190327...\n",
      "Already processed 20190328...\n",
      "Already processed 20190329...\n",
      "Already processed 20190330...\n",
      "Already processed 20190331...\n",
      "Already processed 20190401...\n",
      "Already processed 20190402...\n",
      "Already processed 20190403...\n",
      "Already processed 20190404...\n",
      "Already processed 20190405...\n",
      "Already processed 20190406...\n",
      "Already processed 20190407...\n",
      "Already processed 20190408...\n",
      "Already processed 20190409...\n",
      "Already processed 20190410...\n",
      "Already processed 20190411...\n",
      "Already processed 20190412...\n",
      "Already processed 20190413...\n",
      "Already processed 20190414...\n",
      "Already processed 20190415...\n",
      "Already processed 20190416...\n",
      "Already processed 20190417...\n",
      "Already processed 20190418...\n",
      "Already processed 20190419...\n",
      "Already processed 20190420...\n",
      "Already processed 20190421...\n",
      "Already processed 20190422...\n",
      "Already processed 20190423...\n",
      "Already processed 20190424...\n",
      "Already processed 20190425...\n",
      "Already processed 20190426...\n",
      "Already processed 20190427...\n",
      "Already processed 20190428...\n",
      "Already processed 20190429...\n",
      "Already processed 20190430...\n",
      "Already processed 20190501...\n",
      "Already processed 20190502...\n",
      "Already processed 20190503...\n",
      "Already processed 20190504...\n",
      "Already processed 20190505...\n",
      "Already processed 20190506...\n",
      "Already processed 20190507...\n",
      "Already processed 20190508...\n",
      "checking 20190509... \n"
     ]
    }
   ],
   "source": [
    "dirs = sorted(list(rootdir.glob(\"2019*\")))  # pathlib, where have you been all my life!\n",
    "csv_header = \"time,wfs,file,exptime,airmass,az,el,osst,outt,chamt,tiltx,tilty,transx,transy,focus,focerr,cc_x_err,cc_y_err,xcen,ycen,seeing,raw_seeing,fwhm,wavefront_rms,residual_rms\\n\"\n",
    "for d in dirs:\n",
    "    if d.is_dir():\n",
    "        if Path.exists(d / \"reanalyze_results.csv\"):\n",
    "            print(\"Already processed %s...\" % d.name)\n",
    "        else:\n",
    "            try:\n",
    "                lines = []\n",
    "                lines.append(csv_header)\n",
    "                night = int(d.name)  # valid WFS directories are ints of the form YYYYMMDD. if not this form, int barfs\n",
    "                msg = \"checking %d... \" % night\n",
    "                fitsfiles = d.glob(\"*.fits\")\n",
    "                print(msg)\n",
    "#                 for f in fitsfiles:\n",
    "#                     print(\"Processing %s...\" % f)\n",
    "#                     process_image(f)\n",
    "                nproc = 6 # my mac mini's i7 has 6 cores and py37 can use hyperthreading for more... \n",
    "                with Pool(processes=nproc) as pool:  # my mac mini's i7 has 6 cores... \n",
    "                    plines = pool.map(process_image, fitsfiles)  # plines comes out in same order as fitslines!\n",
    "                plines = list(filter(None.__ne__, plines))  # trim out any None entries\n",
    "                lines.extend(plines)\n",
    "                with open(d / \"reanalyze_results.csv\", \"w\") as f:\n",
    "                    f.writelines(lines)\n",
    "            except ValueError as e:  # this means running int(d.name) failed so it's not a valid directory...\n",
    "                print(f\"Skipping %s... ({e})\" % d.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_image(Path(\"/Volumes/LaCie 8TB/wfsdat/20190423/manual_wfs_0005.fits\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<i>Table length=14</i>\n",
       "<table id=\"table120723902816\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>time</th><th>wfs</th><th>file</th><th>exptime</th><th>airmass</th><th>tiltx</th><th>tilty</th><th>transx</th><th>transy</th><th>focus</th><th>focerr</th><th>cc_x_err</th><th>cc_y_err</th><th>xcen</th><th>ycen</th><th>seeing</th><th>raw_seeing</th><th>fwhm</th><th>wavefront_rms</th><th>residual_rms</th><th>az</th><th>el</th><th>osst</th><th>outt</th><th>chamt</th></tr></thead>\n",
       "<thead><tr><th>str26</th><th>str5</th><th>str26</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th></tr></thead>\n",
       "<tr><td>2019-01-07T08:42:16.357000</td><td>newf9</td><td>f9wfs_20190107-014234.fits</td><td>10.0</td><td>1.0103</td><td>262.15</td><td>67.62</td><td>760.62</td><td>1263.54</td><td>826.94</td><td>174.6</td><td>33.823</td><td>69.341</td><td>1754.302456</td><td>1459.550976</td><td>3.061134</td><td>3.080013</td><td>23.54969</td><td>8306.474561</td><td>0.013232</td><td>32.06631225</td><td>81.82754318</td><td>4.88</td><td>-0.2</td><td>0.135</td></tr>\n",
       "<tr><td>2019-01-07T08:45:56.894000</td><td>newf9</td><td>f9wfs_20190107-014608.fits</td><td>10.0</td><td>1.0093</td><td>261.77</td><td>66.65</td><td>767.54</td><td>1256.75</td><td>849.24</td><td>214.36</td><td>-0.0</td><td>-3.288</td><td>444.093795</td><td>374.430704</td><td>1.151006</td><td>1.157416</td><td>10.658541</td><td>4436.893868</td><td>2776.675973</td><td>27.82345466</td><td>82.2030739</td><td>4.45</td><td>-0.1</td><td>0.207</td></tr>\n",
       "<tr><td>2019-01-07T08:47:37.230000</td><td>newf9</td><td>f9wfs_20190107-014809.fits</td><td>30.0</td><td>1.0089</td><td>202.91</td><td>64.33</td><td>781.76</td><td>909.27</td><td>1070.32</td><td>-21.56</td><td>-0.0</td><td>-0.645</td><td>381.564102</td><td>448.565931</td><td>1.165279</td><td>1.171491</td><td>10.760471</td><td>847.297267</td><td>2468.092629</td><td>25.24997173</td><td>82.39181557</td><td>4.27</td><td>0.0</td><td>0.195</td></tr>\n",
       "<tr><td>2019-01-07T08:49:01.225000</td><td>newf9</td><td>f9wfs_20190107-014907.fits</td><td>5.0</td><td>1.0075</td><td>202.39</td><td>62.98</td><td>791.49</td><td>899.61</td><td>1079.77</td><td>-18.28</td><td>-2.011</td><td>-7.612</td><td>407.289992</td><td>429.536866</td><td>1.089587</td><td>1.094483</td><td>10.20092</td><td>1091.069857</td><td>2538.051729</td><td>28.94242932</td><td>82.98480906</td><td>4.08</td><td>0.0</td><td>0.233</td></tr>\n",
       "<tr><td>2019-01-07T08:50:34.773000</td><td>newf9</td><td>f9wfs_20190107-015042.fits</td><td>5.0</td><td>1.0334</td><td>209.35</td><td>81.0</td><td>665.24</td><td>1023.47</td><td>1082.29</td><td>-28.31</td><td>0.0</td><td>0.0</td><td>356.397331</td><td>463.74719</td><td>1.095665</td><td>1.117478</td><td>10.368482</td><td>926.229457</td><td>2558.610657</td><td>12.46391385</td><td>75.39840635</td><td>3.93</td><td>0.4</td><td>0.448</td></tr>\n",
       "<tr><td>2019-01-07T08:50:51.949000</td><td>newf9</td><td>f9wfs_20190107-015123.fits</td><td>30.0</td><td>1.0332</td><td>209.71</td><td>81.75</td><td>659.74</td><td>1029.2</td><td>1080.5</td><td>-23.89</td><td>-0.0</td><td>0.0</td><td>364.131662</td><td>456.680347</td><td>1.175884</td><td>1.199154</td><td>10.960383</td><td>846.156809</td><td>2534.172338</td><td>11.99886174</td><td>75.42877968</td><td>3.85</td><td>0.6</td><td>0.613</td></tr>\n",
       "<tr><td>2019-01-07T08:52:20.109000</td><td>newf9</td><td>f9wfs_20190107-015252.fits</td><td>30.0</td><td>1.0329</td><td>209.61</td><td>81.65</td><td>660.87</td><td>1027.74</td><td>1069.21</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>369.464652</td><td>449.093992</td><td>1.192806</td><td>1.216199</td><td>11.083281</td><td>692.108567</td><td>2472.071811</td><td>11.00509259</td><td>75.48959072</td><td>3.7</td><td>0.8</td><td>0.762</td></tr>\n",
       "<tr><td>2019-01-07T08:53:46.527000</td><td>newf9</td><td>f9wfs_20190107-015418.fits</td><td>30.0</td><td>1.0327</td><td>209.55</td><td>81.45</td><td>661.93</td><td>1026.7</td><td>1117.59</td><td>-6.04</td><td>0.0</td><td>0.0</td><td>378.922652</td><td>444.197907</td><td>1.144793</td><td>1.167109</td><td>10.728755</td><td>487.315068</td><td>2368.813984</td><td>10.02608633</td><td>75.54372843</td><td>3.53</td><td>0.8</td><td>0.757</td></tr>\n",
       "<tr><td>2019-01-07T09:14:58.950000</td><td>newf9</td><td>f9wfs_20190107-021531.fits</td><td>30.0</td><td>1.0118</td><td>203.92</td><td>67.14</td><td>761.78</td><td>928.25</td><td>930.35</td><td>236.6</td><td>2.224</td><td>2.762</td><td>400.061331</td><td>436.199283</td><td>0.91114</td><td>0.917575</td><td>8.897915</td><td>4902.102237</td><td>1610.297323</td><td>341.01308273</td><td>81.24046129</td><td>2.4</td><td>1.4</td><td>1.197</td></tr>\n",
       "<tr><td>2019-01-07T09:16:47.011000</td><td>newf9</td><td>f9wfs_20190107-021718.fits</td><td>30.0</td><td>1.0122</td><td>204.74</td><td>69.34</td><td>734.78</td><td>940.86</td><td>928.89</td><td>243.47</td><td>0.468</td><td>2.749</td><td>399.427021</td><td>427.634137</td><td>0.831735</td><td>0.837809</td><td>8.302147</td><td>5053.595068</td><td>1562.536281</td><td>338.95826391</td><td>81.10851807</td><td>2.32</td><td>1.4</td><td>1.169</td></tr>\n",
       "<tr><td>2019-01-07T09:18:14.692000</td><td>newf9</td><td>f9wfs_20190107-021846.fits</td><td>30.0</td><td>1.0125</td><td>204.92</td><td>71.32</td><td>709.19</td><td>942.73</td><td>966.99</td><td>249.47</td><td>0.731</td><td>0.0</td><td>399.571846</td><td>428.440023</td><td>0.865069</td><td>0.871541</td><td>8.554724</td><td>5079.165429</td><td>1532.197355</td><td>337.31824334</td><td>80.99065108</td><td>2.27</td><td>1.4</td><td>1.116</td></tr>\n",
       "<tr><td>2019-01-07T12:14:17.950000</td><td>newf9</td><td>f9wfs_20190107-051439.fits</td><td>20.0</td><td>1.3019</td><td>238.25</td><td>135.01</td><td>292.16</td><td>1405.03</td><td>897.79</td><td>213.52</td><td>-6.648</td><td>-0.0</td><td>435.403182</td><td>416.741114</td><td>0.876452</td><td>1.026774</td><td>9.705156</td><td>4370.58638</td><td>1355.641798</td><td>296.23688733</td><td>50.18621085</td><td>1.15</td><td>1.6</td><td>0.854</td></tr>\n",
       "<tr><td>2019-01-07T12:15:31.833000</td><td>newf9</td><td>f9wfs_20190107-051554.fits</td><td>20.0</td><td>1.3064</td><td>197.03</td><td>118.05</td><td>391.58</td><td>1120.41</td><td>866.66</td><td>254.86</td><td>-0.048</td><td>-0.0</td><td>376.066042</td><td>437.634932</td><td>0.877254</td><td>1.029843</td><td>9.727703</td><td>5187.077939</td><td>1393.733872</td><td>296.26238977</td><td>49.94592784</td><td>1.18</td><td>1.8</td><td>0.993</td></tr>\n",
       "<tr><td>2019-01-07T12:16:44.726000</td><td>newf9</td><td>f9wfs_20190107-051706.fits</td><td>20.0</td><td>1.3109</td><td>197.36</td><td>118.36</td><td>389.63</td><td>1123.58</td><td>870.35</td><td>255.94</td><td>-3.296</td><td>-0.0</td><td>374.213961</td><td>436.375163</td><td>0.870926</td><td>1.024526</td><td>9.688636</td><td>5171.411575</td><td>1378.821825</td><td>296.28793513</td><td>49.71536516</td><td>1.18</td><td>1.9</td><td>1.13</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Table length=14>\n",
       "           time             wfs             file            ...   outt   chamt \n",
       "          str26             str5           str26            ... float64 float64\n",
       "-------------------------- ----- -------------------------- ... ------- -------\n",
       "2019-01-07T08:42:16.357000 newf9 f9wfs_20190107-014234.fits ...    -0.2   0.135\n",
       "2019-01-07T08:45:56.894000 newf9 f9wfs_20190107-014608.fits ...    -0.1   0.207\n",
       "2019-01-07T08:47:37.230000 newf9 f9wfs_20190107-014809.fits ...     0.0   0.195\n",
       "2019-01-07T08:49:01.225000 newf9 f9wfs_20190107-014907.fits ...     0.0   0.233\n",
       "2019-01-07T08:50:34.773000 newf9 f9wfs_20190107-015042.fits ...     0.4   0.448\n",
       "2019-01-07T08:50:51.949000 newf9 f9wfs_20190107-015123.fits ...     0.6   0.613\n",
       "2019-01-07T08:52:20.109000 newf9 f9wfs_20190107-015252.fits ...     0.8   0.762\n",
       "2019-01-07T08:53:46.527000 newf9 f9wfs_20190107-015418.fits ...     0.8   0.757\n",
       "2019-01-07T09:14:58.950000 newf9 f9wfs_20190107-021531.fits ...     1.4   1.197\n",
       "2019-01-07T09:16:47.011000 newf9 f9wfs_20190107-021718.fits ...     1.4   1.169\n",
       "2019-01-07T09:18:14.692000 newf9 f9wfs_20190107-021846.fits ...     1.4   1.116\n",
       "2019-01-07T12:14:17.950000 newf9 f9wfs_20190107-051439.fits ...     1.6   0.854\n",
       "2019-01-07T12:15:31.833000 newf9 f9wfs_20190107-051554.fits ...     1.8   0.993\n",
       "2019-01-07T12:16:44.726000 newf9 f9wfs_20190107-051706.fits ...     1.9    1.13"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = Path(\"/Volumes/Seagate2TB/wfsdat/20190107/reanalyze_results.csv\")\n",
    "t = ascii.read(f)\n",
    "az = Column(np.zeros(len(t)), name='az')\n",
    "el = Column(np.zeros(len(t)), name='el')\n",
    "osst = Column(np.zeros(len(t)), name='osst')\n",
    "outt = Column(np.zeros(len(t)), name='outt')\n",
    "chamt = Column(np.zeros(len(t)), name='chamt')\n",
    "t.add_column(az)\n",
    "t.add_column(el)\n",
    "t.add_column(osst)\n",
    "t.add_column(outt)\n",
    "t.add_column(chamt)\n",
    "for r in t:\n",
    "    with fits.open(f.parent / r['file']) as hl:\n",
    "        hdr = hl[-1].header\n",
    "        if np.isnan(r['focus']):\n",
    "            focus = hdr.get('TRANSZ', np.nan)\n",
    "            r['focus'] = focus\n",
    "        a = hdr.get('AZ', np.nan)\n",
    "        if a < 0:\n",
    "            a += 360.\n",
    "        r['az'] = a\n",
    "        r['el'] = hdr.get('EL', np.nan)\n",
    "        r['osst'] = hdr.get('OSSTEMP', np.nan)\n",
    "        r['outt'] = hdr.get('OUT_T', np.nan)\n",
    "        r['chamt'] = hdr.get('CHAM_T', np.nan)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing /Volumes/LaCie 8TB/wfsdat/20190101/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190102/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190103/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190104/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190105/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190106/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190107/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190108/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190109/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190110/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190111/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190112/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190113/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190114/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190115/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190116/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190117/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190118/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190119/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190120/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190121/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190122/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190123/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190124/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190125/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190126/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190127/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190128/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190129/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190130/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190131/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190201/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190202/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190203/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190204/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190205/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190206/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190207/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190208/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190209/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190210/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190211/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190212/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190213/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190214/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190215/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190216/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190217/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190218/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190219/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190220/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190221/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190222/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190223/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190224/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190225/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190226/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190227/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190228/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190301/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190302/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190303/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190304/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190305/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190306/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190307/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190308/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190309/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190310/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190311/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190312/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190313/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190314/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190315/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190316/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190317/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190318/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190319/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190320/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190321/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190322/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190323/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190324/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190325/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190326/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190327/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190328/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190329/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190330/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190331/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190401/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190402/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190403/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190404/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190405/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190406/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190407/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190408/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190409/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190410/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190411/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190412/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190413/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190414/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190415/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190416/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190417/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190418/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190419/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190420/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190421/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190422/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190423/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190424/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190425/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190426/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190427/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190428/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190429/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190430/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190501/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190502/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190503/reanalyze_results.csv...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing /Volumes/LaCie 8TB/wfsdat/20190504/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190505/reanalyze_results.csv...\n",
      "fixing /Volumes/LaCie 8TB/wfsdat/20190506/reanalyze_results.csv...\n"
     ]
    }
   ],
   "source": [
    "for d in dirs:\n",
    "    if d.is_dir():\n",
    "        if Path.exists(d / \"reanalyze_results.csv\"):\n",
    "            f = d / \"reanalyze_results.csv\"\n",
    "            print(f\"fixing {f}...\")\n",
    "            t = ascii.read(f)\n",
    "            #osst = Column(np.zeros(len(t)), name='osst')\n",
    "            #outt = Column(np.zeros(len(t)), name='outt')\n",
    "            #chamt = Column(np.zeros(len(t)), name='chamt')\n",
    "            #t.add_column(osst)\n",
    "            #t.add_column(outt)\n",
    "            #t.add_column(chamt)\n",
    "            for r in t:\n",
    "                with fits.open(f.parent / r['file']) as hl:\n",
    "                    hdr = hl[0].header\n",
    "                    if np.isnan(r['focus']):\n",
    "                        focus = hdr.get('TRANSZ', np.nan)\n",
    "                        r['focus'] = focus\n",
    "                    a = hdr.get('AZ', np.nan)\n",
    "                    if a < 0:\n",
    "                        a += 360.\n",
    "                    r['az'] = a\n",
    "                    r['el'] = hdr.get('EL', np.nan)\n",
    "                    r['osst'] = hdr.get('OSSTEMP', np.nan)\n",
    "                    if 'OUT_T' in hdr:\n",
    "                        r['outt'] = hdr.get('OUT_T', np.nan)\n",
    "                    else:\n",
    "                        r['outt'] = hdr.get('T_OUT', np.nan)\n",
    "                    if 'CHAM_T' in hdr:\n",
    "                        r['chamt'] = hdr.get('CHAM_T', np.nan)\n",
    "                    else:\n",
    "                        r['chamt'] = hdr.get('T_CHAM', np.nan)\n",
    "            t.write(f, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "print(\"1\\\n",
    "2\\\n",
    "3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
